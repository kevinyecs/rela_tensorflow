# ReLA - TensorFlow
Rectified Linear Attention (TensorFlow implementation) from the paper [Sparse Attention with Linear Units](https://arxiv.org/pdf/2104.07012.pdf)

## Roadmap
- [] ReLA module, Transformer Model


> [!WARNING]
> This repository is under developemnt, but please feel free to explore and provide any feedback or suggestions you may have. :construction:

## Citations

```bibtex
@misc{zhang2021sparse,
      title={Sparse Attention with Linear Units}, 
      author={Biao Zhang and Ivan Titov and Rico Sennrich},
      year={2021},
      eprint={2104.07012},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```
