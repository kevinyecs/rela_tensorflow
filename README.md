# ReLA - TensorFlow
Rectified Linear Attention (TensorFlow implementation) from the paper [Sparse Attention with Linear Units](https://arxiv.org/pdf/2104.07012.pdf)

## Roadmap
- [ ] ReLA module, Transformer Model

> [!WARNING]
> This repository is under developemnt, but please feel free to explore and provide any feedback or suggestions you may have. :construction:

## Citations

```bibtex
@article{Zhang2021SparseAwLU,
    title   = {Sparse Attention with Linear Units}, 
    author  = {Biao Zhang and Ivan Titov and Rico Sennrich},
    journal = {ArXiv},
    year    = {2021},
    volume  = {abs/2104.07012}
}
```
